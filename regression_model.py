# -*- coding: utf-8 -*-
"""kenan_yilmaz_makine_ogr_proje_3ocak.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11OK9iqxNsRUsdqP06NtA8bdQGk3lhlOg
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import f_regression
from scipy.stats import zscore
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE

df = pd.read_csv('btc_veri.csv')
df.info()

print(df['Volatility_24h'].value_counts())

from sklearn.preprocessing import MinMaxScaler

# Min-Max Scaler tanımla
scaler = MinMaxScaler()

# Volatility_24h sütununu ölçeklendir
df['Volatility_24h'] = scaler.fit_transform(df[['Volatility_24h']])

# Yeni sütunu kontrol et
print(df[['Volatility_24h', 'Volatility_24h']].head())

# Close ve Open Time sütunlarını seç
df["Open Time"] = pd.to_datetime(df["Open Time"])  # Tarih formatına çevir
df["Volatility_24h"] = df["Volatility_24h"].astype(float)  # Float türüne çevir

# Close verisinin grafiğini çiz
plt.figure(figsize=(12, 6))
plt.plot(df["Open Time"], df["Volatility_24h"], label="Volatility_24h", linewidth=1.5)

# Grafik ayarları
plt.title("BTC Volalite", fontsize=16)
plt.xlabel("Zaman", fontsize=12)
plt.ylabel("0-1 aralığına ölçeklendirilmiş volalite", fontsize=12)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

df.info()



# obje değerler hariç df2 ye atama
df2 = df.select_dtypes(exclude=['object'])
df2.head()

# korelaasyon matrisine bakma ve Volatility_24h ile korelasyon olanları tespit etme
corr_matrix = df2.corr()
corr_matrix['Volatility_24h'].sort_values(ascending=False)

# Korelasyon matrisini hesapla
corr_matrix = df2.corr()

# Volatility_24h ile yüksek korelasyona sahip sütunları bul
correlation_threshold = 0.2 # Eşik değeri (pozitif veya negatif korelasyon için)
high_corr_columns = corr_matrix.index[(corr_matrix['Volatility_24h'] >= correlation_threshold) |
                                      (corr_matrix['Volatility_24h'] <= -correlation_threshold)].tolist()

# Yeni bir DataFrame'e yüksek korelasyonlu sütunları kaydet
df3 = df2[high_corr_columns]

# df3'ü kaydetmek isterseniz
df3.to_csv("high_correlation_with_volatility.csv", index=False)

print(f"Volatility_24h ile yüksek korelasyona sahip sütunlar: {high_corr_columns}")

df3.info()

# null değerleri bulma
df3.isnull().sum()

# sutunlara özel null değerleri doldurma
# Ortalamayı doldurmak
df3['Low Price'].fillna(df3['Low Price'].mean(), inplace=True)

# Medyanla doldurmak
df3['Volume'].fillna(df3['Volume'].median(), inplace=True)

# Son bilinen değeri taşıma (forward fill)
df3['MACD'].fillna(method='ffill', inplace=True)

# Interpolasyon
df3['RSI_24h'].interpolate(method='linear', inplace=True)

# Yeniden hesaplama (örneğin, hareketli ortalamalar için)
df3['Volume_MA4'] = df3['Volume'].rolling(window=4, min_periods=1).mean()

# Medyan ile doldurma
df3['Quote Asset Volume'].fillna(df3['Quote Asset Volume'].median(), inplace=True)

# Son bilinen değeri taşıma (forward fill)
df3['Number of Trades'].fillna(method='ffill', inplace=True)

# Interpolasyon
df3['Volatility_4h'].interpolate(method='linear', inplace=True)
df3['ATR_4h'].interpolate(method='linear', inplace=True)
df3['RSI_24h'].interpolate(method='linear', inplace=True)

# Hareketli ortalamalar için yeniden hesaplama
df3['Volume_MA4'] = df3['Volume'].rolling(window=4, min_periods=1).mean()
df3['Volume_MA8'] = df3['Volume'].rolling(window=8, min_periods=1).mean()
df3['Volume_MA24'] = df3['Volume'].rolling(window=24, min_periods=1).mean()

# Standart sapma yeniden hesaplama
df3['Price_StdDev_4h'] = df3['Low Price'].rolling(window=4, min_periods=1).std()
df3['Volume_StdDev_4h'] = df3['Volume'].rolling(window=4, min_periods=1).std()

# Kalan null olan sutunları çıkarma
df3.dropna(inplace=True)

# null kontrol
df3.info()

# Hedef ve bağımsız değişkenlerin ayrılması
X = df3.drop("Volatility_24h", axis=1)
y = df3["Volatility_24h"]

# 1. Z-Skoru ile Uç Değerlerin Temizlenmesi
z_scores = np.abs(zscore(X))
outliers = (z_scores > 3).any(axis=1)
X_cleaned = X[~outliers]
y_cleaned = y[~outliers]

# 2. Korelasyon Analizi ve görsel olarak gösterilmesi
correlation_matrix = X_cleaned.corrwith(y_cleaned)
print("Korelasyon Analizi Sonuçları:")
print(correlation_matrix.sort_values(ascending=False))

# Korelasyon analizi (bağımsız değişkenlerle hedef değişken arasında)
correlation_matrix = X_cleaned.corrwith(y_cleaned)

# Korelasyon sonuçlarının görselleştirilmesi
plt.figure(figsize=(10, 6))
sns.barplot(x=correlation_matrix.index, y=correlation_matrix.values)
plt.title("Korelasyon Analizi (Bağımsız Değişkenler vs Hedef Değişken)")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Korelasyon Değeri")
plt.xlabel("Bağımsız Değişkenler")
plt.grid(axis='y')
plt.tight_layout()
plt.show()



# 4. Feature Importance (Random Forest)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cleaned)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_scaled, y_cleaned)

feature_importance = pd.DataFrame({
    "Feature": X_cleaned.columns,
    "Importance": rf_model.feature_importances_
})

print("\nRandom Forest Feature Importance Sonuçları:")
print(feature_importance.sort_values(by="Importance", ascending=False))

# En anlamlı özelliklerin seçilmesi
selected_features = feature_importance[feature_importance["Importance"] > 0.006]["Feature"].tolist()
X_selected = X_cleaned[selected_features]

# Seçilen özelliklerin kaydedilmesi
df3_selected = pd.concat([X_selected, y_cleaned], axis=1)

df3_selected.to_csv("selected_volatility.csv", index=False)

# 3. Varyans Analizi (F-Test)
f_values, p_values = f_regression(X_selected, y_cleaned)
varyans_analizi = pd.DataFrame({
    "Feature": X_selected.columns,
    "F-Value": f_values,
    "P-Value": p_values
})

print("\nVaryans Analizi (F-Test) Sonuçları:")
print(varyans_analizi.sort_values(by="P-Value"))

# benzer şeyleri ifade eden birden çok   sutunun korelasyon ,varyans analizi ve onem skoruna göre çıkarılması (BB_Lower_4h,BB_Lower_8h,Volatility_8h)

X_selected  = X_selected.drop(["BB_Lower_4h", "BB_Lower_8h", "Volatility_8h"], axis=1)

# volatility_24h verisibe aşırı benzeyen  sutun çıkarılması (ATR_24h)
X_selected  = X_selected.drop(["ATR_24h"], axis=1)

# Seçilen özelliklerin kaydedilmesi
print("\nSeçilen özellikler CSV dosyasına kaydedildi.")
df3_selected = pd.concat([X_selected, y_cleaned], axis=1)

df3_selected.to_csv("selected_volatility.csv", index=False)

print("\nSeçilen özellikler CSV dosyasına kaydedildi.")

data = pd.read_csv('selected_volatility.csv')
data.info()

data.head()

# Hedef ve bağımsız değişkenlerin ayrılması
X = data.drop("Volatility_24h", axis=1)
y = data["Volatility_24h"]


# 2. Eğitim ve Test Setlerine Ayırma
X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)

# 3. Verilerin Ölçeklendirilmesi
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Makine Öğrenmesi Modelleri
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Support Vector Regressor": SVR(kernel="rbf"),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

# 6. Modellerin Eğitilmesi ve Değerlendirilmesi
results = {}
for model_name, model in models.items():
    # Modeli eğit
    model.fit(X_train_scaled, y_train)

    # Tahmin yap
    y_pred = model.predict(X_test_scaled)

    # Değerlendirme metrikleri
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[model_name] = {"MSE": mse, "R2": r2}

# 7. Sonuçları Analiz Et
print("\nModel Performans Karşılaştırması:")
for model_name, metrics in results.items():
    print(f"{model_name}: MSE = {metrics['MSE']:.8f}, R2 = {metrics['R2']:.4f}")

# 8. En İyi Modeli Kaydetme
best_model_name = max(results, key=lambda k: results[k]["R2"])
print(f"\nEn iyi model: {best_model_name}")
